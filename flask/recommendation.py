"""
Supervised learning project to reverse engineer recommended games from GiantBomb API
"""
import requests_cache
import pdb
import json
import os
from dotenv import load_dotenv
import time
from nltk.tokenize import RegexpTokenizer
from nltk.corpus import stopwords
import random
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

import tensorflow as tf
import tensorflow_hub as hub
from sklearn.svm import SVC
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import f1_score
from sklearn.metrics import RocCurveDisplay

from sklearn.decomposition import PCA
from mlxtend.plotting import plot_decision_regions

from get_api_info import get_giantbomb_game_info, get_gamespot_games, get_similar_games
from process_recs import process_text, check_for_valid_qualities, check_valid_deck_and_desc, get_embedding_similarity, get_embedding
#from calculate_metrics import calculate_confusion_matrix, calculate_average_pairs

start_time = time.time()
load_dotenv()
session = requests_cache.CachedSession("recommendation_cache")

# http://www.gamespot.com/api/games/?api_key=<YOUR_API_KEY_HERE>
GAMESPOT_API_KEY = os.getenv('GAMESPOT_API_KEY')
HEADERS = {'User-Agent': 'Video Game Recommender Comprehensive Project'}
GIANTBOMB_API_KEY = os.getenv('GIANTBOMB_API_KEY')

# https://www.kaggle.com/datasets/dahlia25/metacritic-video-game-comments - use metacritic_game_info.csv to parse
csv_titles_df = pd.read_csv("flask/metacritic_game_info.csv")

"""
1a. form a dataset by combining games from metacritic csv and GameSpot API (all recommendation boolean == 0)
Get titles from each of the data sources, then get information from API calls
"""
csv_titles = list(set([i for i in csv_titles_df['Title']][0:100])) # 10 games
print(csv_titles[0])
print(len(csv_titles))
pdb.set_trace()

dataset = {}
for title in csv_titles:
    query_dict = get_giantbomb_game_info(api_key=GIANTBOMB_API_KEY, query=title, headers=HEADERS,session=session)
    dataset = {**dataset, **query_dict}

pdb.set_trace()

#print(dataset)
print("investigate dataset")
#pdb.set_trace()

# get gamespot games - (2 * 99) games before filtering
gamespot_games = get_gamespot_games(api_key=GAMESPOT_API_KEY, headers=HEADERS, game_count=3, session=session)

dataset = {**dataset, **gamespot_games}
print("make sure all csv + gamespot games are recommended==0")
pdb.set_trace()

"""
1b. get similar_games depending on the dataset items (all recommendation boolean == 1)
Iterate through the dataset and find similar games via API call, assuming they exist
"""
similar_games_dict = {}
for k1, v1 in dataset.items():
    # FIXME remove max_similar later
    similar_games_instance = get_similar_games(api_key=GIANTBOMB_API_KEY, query=k1, headers=HEADERS, max_similar=10, session=session)
    if similar_games_instance == None or similar_games_instance == {}:
        continue
    else:
        similar_games_dict = {**similar_games_dict, **similar_games_instance}

print("check similar_games")
pdb.set_trace()

print("check len dataset vs. len similar_games")
pdb.set_trace()

"""
1c. combine recommendation boolean == 0 items (csv and gamespot) with boolean == 1 items (similar games)
This forms the dataset of games which will be used for training the model
"""
dataset = {**dataset, **similar_games_dict}

# randomly shuffle dictionary keys to mix ground truth games with games_dict
# https://stackoverflow.com/questions/19895028/randomly-shuffling-a-dictionary-in-python
temp_list = list(dataset.items())
random.shuffle(temp_list)
total_games_dict = dict(temp_list)

print("check total_games")
pdb.set_trace()

"""
2. Generate a train and test set for the model using previous API calls (with recommendation boolean=0)
and similar_games (recommendation_boolean=1) 
"""
# set up tf universal encoder for cosine similarity comparisons on sentence embeddings
module_url = "https://tfhub.dev/google/universal-sentence-encoder/4"
model = hub.load(module_url)
print(type(model))
print("tf universal encoder set up!")

# X is a list of deck and description embeddings, as generated by encoder
# y is the recommendation boolean
X = []
y = []

for k, v in dataset.items():
    deck = v['deck']
    desc = v['description']

    if check_valid_deck_and_desc(deck, desc) == False:
        continue

    tokenized_deck = process_text(deck)
    tokenized_desc = process_text(desc)
    tokenized_list = tokenized_deck + tokenized_desc
    word_embedding = get_embedding(model, tokenized_list)

    X.append(word_embedding)
    y.append(v['recommended'])

print("check dataset X and y")
pdb.set_trace()

# train test split - 80% train, 20% test
split = int(0.8 * len(X))
X_train = np.array(X[0:split])
y_train = np.array(y[0:split])
X_test = np.array(X[split:])
y_test = np.array(y[split:])

print("check train/test split")
pdb.set_trace()

"""
3. Feed train and test set into SVM and evaluate the model.
SVM is chosen because it works well with high-dimensional, natural language-driven data
"""
clf = make_pipeline(StandardScaler(), SVC(gamma='auto', kernel='rbf'))
clf.fit(X_train, y_train)

y_preds = clf.predict(X_test)
print("F-1 score: ", f1_score(y_test, y_preds, average='binary'))

RocCurveDisplay.from_estimator(clf, X_test, y_test)
lin_x = np.linspace(0.0, 1.0, 11)
lin_y = np.linspace(0.0, 1.0, 11)
plt.plot(lin_x, lin_y, label='linear')  # Plot some data on the (implicit) axes.
plt.xlabel("FPR")
plt.ylabel("TPR")
plt.title("ROC curve")
plt.show()

print("check evaluations")
pdb.set_trace()

"""
4. Apply principal component analysis to reduce the dimension of the input word embeddings
This will make the decision boundary easier to visualize (2 dimensions rather than N-space)
"""
pca = PCA(n_components = 2)
X_lowdim = pca.fit_transform(X)
clf.fit(X_lowdim, y)

plot_decision_regions(np.array(X_lowdim), np.array(y), clf=clf, legend=2)

# Adding axes annotations
plt.xlabel('Word embedding')
plt.title('SVM Visualization')
plt.show()

print("check visualizations")
pdb.set_trace()

print("final pdb")
pdb.set_trace()

print("final time")
finaltime = time.time() - start_time
print("final time (min): ", finaltime/60)

# FIXME fix data sparseness (try SMOTE)