"""
Supervised learning project to reverse engineer recommended games from GiantBomb API
"""
import requests_cache
import pdb
import json
import os
from dotenv import load_dotenv
import time
from nltk.tokenize import RegexpTokenizer
from nltk.corpus import stopwords
import random
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

import tensorflow as tf
import tensorflow_hub as hub
from sklearn.svm import SVC
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import f1_score
from sklearn.metrics import RocCurveDisplay

from sklearn.decomposition import PCA
from mlxtend.plotting import plot_decision_regions

from get_api_info import get_giantbomb_game_info, get_gamespot_games, get_similar_games
from process_recs import process_text, check_for_valid_qualities, check_valid_deck_and_desc, get_embedding_similarity, get_embedding

start_time = time.time()
load_dotenv()
session = requests_cache.CachedSession("recommendation_cache")

# http://www.gamespot.com/api/games/?api_key=<YOUR_API_KEY_HERE>
GAMESPOT_API_KEY = os.getenv('GAMESPOT_API_KEY')
HEADERS = {'User-Agent': 'Video Game Recommender Comprehensive Project'}
GIANTBOMB_API_KEY = os.getenv('GIANTBOMB_API_KEY')

# https://www.kaggle.com/datasets/dahlia25/metacritic-video-game-comments - use metacritic_game_info.csv to parse
csv_titles_df = pd.read_csv("flask/metacritic_game_info.csv")

"""
1. Form a query set by combining games from metacritic csv and GameSpot API (all recommendation boolean == 0)
Get titles from each of the data sources, then get information from API calls
"""
csv_titles = list(set([i for i in csv_titles_df['Title']][0:15]))
#csv_titles = list(set([i for i in csv_titles_df['Title']][0:100])) # 10 games
print(csv_titles[0])
print(len(csv_titles))
pdb.set_trace()

query_set = {}
for title in csv_titles:
    query_dict = get_giantbomb_game_info(api_key=GIANTBOMB_API_KEY, query=title, headers=HEADERS,session=session)
    query_set = {**query_set, **query_dict}

pdb.set_trace()

# get gamespot games - (2 * 99) games before filtering
gamespot_games = get_gamespot_games(api_key=GAMESPOT_API_KEY, headers=HEADERS, game_count=2, session=session)

query_set = {**query_set, **gamespot_games}

"""
similar_games_dict = {}
for k, v in query_set.items():

    # FIXME remove max_similar later
    similar_games_instance = get_similar_games(api_key=GIANTBOMB_API_KEY, query=k, headers=HEADERS, max_similar=10, session=session)
    if similar_games_instance == None or similar_games_instance == {}:
        continue

    for sk, sv in similar_games_instance.items():
        
    
    

"""

"""
2. Generate a train and test set for the model 
"""
# set up tf universal encoder for cosine similarity comparisons on sentence embeddings
module_url = "https://tfhub.dev/google/universal-sentence-encoder/4"
model = hub.load(module_url)
print(type(model))
print("tf universal encoder set up!")

# X is a list of deck and description embeddings, as generated by encoder
# y is the recommendation boolean
X = []
y = []

for k, v in query_set.items():

    # on a per game basis,
    # check similar games (which are recommended == 1 contingent on the query set game)
    # and check all other query set games (which are recommended == 0 since they are not similar)
    similar_games_instance = get_similar_games(api_key=GIANTBOMB_API_KEY, query=k, headers=HEADERS, max_similar=10, session=session)
    if similar_games_instance == None or similar_games_instance == {}:
        continue
    
    for sk, sv in similar_games_instance.items():
        deck = sv['deck']
        desc = sv['description']

        if check_valid_deck_and_desc(deck, desc) == False:
            continue

        tokenized_deck = process_text(deck)
        tokenized_desc = process_text(desc)
        tokenized_list = tokenized_deck + tokenized_desc
        word_embedding = get_embedding(model, tokenized_list)

        X.append(word_embedding)
        y.append(1)
    
    #not_similar_games_instance = [{k, v} for k, v in query_set.items() if k not in similar_games_instance.keys()]
    #not_similar_games_instance = not_similar_games_instance[0]

    for nk, nv in query_set.items():
        if nk in similar_games_instance:
            continue

        deck = nv['deck']
        desc = nv['description']

        if check_valid_deck_and_desc(deck, desc) == False:
            continue

        tokenized_deck = process_text(deck)
        tokenized_desc = process_text(desc)
        tokenized_list = tokenized_deck + tokenized_desc
        word_embedding = get_embedding(model, tokenized_list)

        X.append(word_embedding)
        y.append(0)

print("check dataset X and y")
pdb.set_trace()

# train test split - 80% train, 20% test
split = int(0.8 * len(X))
X_train = np.array(X[0:split])
y_train = np.array(y[0:split])
X_test = np.array(X[split:])
y_test = np.array(y[split:])

print("check train/test split")
pdb.set_trace()

print("check if smote needed")
print(y.count(0))
print(y.count(1))
pdb.set_trace()

"""
3. Feed train and test set into SVM and evaluate the model.
SVM is chosen because it works well with high-dimensional, natural language-driven data
"""
clf = make_pipeline(StandardScaler(), SVC(gamma='auto', kernel='rbf'))
clf.fit(X_train, y_train)

y_preds = clf.predict(X_test)
print("F-1 score: ", f1_score(y_test, y_preds, average='binary'))

RocCurveDisplay.from_estimator(clf, X_test, y_test)
lin_x = np.linspace(0.0, 1.0, 11)
lin_y = np.linspace(0.0, 1.0, 11)
plt.plot(lin_x, lin_y, label='linear')  # Plot some data on the (implicit) axes.
plt.xlabel("FPR")
plt.ylabel("TPR")
plt.title("ROC curve")
plt.show()

print("check evaluations")
pdb.set_trace()

"""
4. Apply principal component analysis to reduce the dimension of the input word embeddings
This will make the decision boundary easier to visualize (2 dimensions rather than N-space)
"""
pca = PCA(n_components = 2)
X_lowdim = pca.fit_transform(X)
clf.fit(X_lowdim, y)

plot_decision_regions(np.array(X_lowdim), np.array(y), clf=clf, legend=2)

# Adding axes annotations
plt.xlabel('Word embedding')
plt.ylabel('Recommendation of game')
plt.title('Support Vector Machine visualization')
plt.show()

print("check visualizations")
pdb.set_trace()

print("final pdb")
pdb.set_trace()

print("final time")
finaltime = time.time() - start_time
print("final time (min): ", finaltime/60)

# FIXME remove artificial limit on similar_games
# FIXME fix data sparseness (try SMOTE) if needed