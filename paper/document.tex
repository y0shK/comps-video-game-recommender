\documentclass[10pt,twocolumn]{article}

% use the oxycomps style file
\usepackage{oxycomps}

% usage: \fixme[comments describing issue]{text to be fixed}
% define \fixme as not doing anything special
\newcommand{\fixme}[2][]{#2}
% overwrite it so it shows up as red
\renewcommand{\fixme}[2][]{\textcolor{red}{#2}}
% overwrite it again so related text shows as footnotes
%\renewcommand{\fixme}[2][]{\textcolor{red}{#2\footnote{#1}}}

% read references.bib for the bibtex data
\bibliography{references}

% include metadata in the generated pdf file
\pdfinfo{
    /Title (Reverse Engineering the GiantBomb API: Using Word Embeddings to Predict Video Game Recommendations)
    /Author (Yash Karandikar)
}

% set the title and author information
\title{Reverse Engineering the GiantBomb API: Using Word Embeddings to Predict Video Game Recommendations}
\author{Yash Karandikar}
\affiliation{Occidental College}
\email{ykarandikar@oxy.edu}

\begin{document}

\maketitle

% FIXME put references in each section
 
\section{Introduction and Problem Context}

%This section should motivate why the project is interesting both to you and to the computer science community or the general public.
%You should also justify the difficult of the project.
%As a rough guideline, your project should be either narrow but deep in a subfield of CS, or broadly reaching across subfields without being too shallow.
%It should be comparable to the amount of work/content in an upper-level elective.

%\begin{comment}
 %   Why the project is interesting.
  %  To me:
   %     - want to gain experience in ML, particularly in setting up a real-world problem (as opposed to problems specifically engineered for textbooks)
    %    - video games are one of my hobbies, and I have seen the various ways sources of video game-related text can arise (reviews, descriptions, forums, etc.) - use NLP to inform ML model
    %To general public:
     %   - recommender systems are prevalent in today's highly technological society. (Buying things on Amazon, watching content on Netflix, connecting with people on Facebook, professional development on LinkedIn, etc. - all use recommendation systems)

    %YouTube recommender system https://dl.acm.org/doi/pdf/10.1145/1864708.1864770
   % Netfix recommender https://dl.acm.org/doi/pdf/10.1145/2843948
   % Linkedin Recommender https://dl.acm.org/doi/pdf/10.1145/3109859.3109921
   % Amazon https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7927889
        
      %  - supervised learning is becoming part of recommender %system infrastructure (particularly for content-based, rather than collaborative filtering-based, aspects)

  %  Difficulty of project:
  %  - ML - deciding, obtaining, and preprocessing data in an ethical, efficient way. This is good practice for the real world
  %  - iterative ML algorithm improvement - how to identify breakpoints and be skeptical of results, then make iterative changes to algorithm to fix results
%\end{comment}

Working on this project is interesting to me because it provides an opportunity to combine machine learning (ML), natural language processing (NLP), and real-world data analysis into one problem. In terms of ML, I want to gain ML experience to be able to apply it in my career going forward (possibly in a healthcare-oriented context). Focusing on video games as the subject for the supervised machine learning problem has also allowed me to tap into an immense amount of text-based video game data (descriptions, reviews, and other qualitative aspects). In this way, I can use NLP ideas as ML features and apply them to a subject of interest. The real-world nature of this problem is also stimulating and challenging, because the data comes from an API that requires authentication and preprocessing rather than a cleanly defined, open-access dataset found in a textbook. These three aspects of the problem have motivated my work in comps and have helped me grow as a developer and machine learning researcher.

On the societal level, recommender systems are critical to today's highly technological, choice-laden society. These recommender systems are found in many different aspects of life. Commercial settings (like Amazon \cite{AmazonRS}), video content platforms (YouTube \cite{YouTubeRS} and Netflix \cite{NetflixRS}), and social media websites (LinkedIn \cite{LinkedInRS}) are just a few examples. Supervised learning (especially self-supervised variants of supervised learning algorithms) is an approach that is explored in the implementation of these recommender systems. Addressing data sparsity, alleviating the "cold start" problem of not having an initial suggestion to base future recommendations on, and including temporal aspects of recommendation as user sentiment changes are all good reasons to apply supervised or self-supervised learning. \cite{JYu, XXin, Matuszyk}

The difficulty of the project stems from the machine learning side of the problem. Because common approaches for recommendation systems require immense amounts of user data and interconnected user profiles, they are not practical for the scope of this project. Instead, transforming the recommendation problem into a supervised learning problem allows me the opportunity to practice real-world machine learning. Deciding which dataset I can use to provide ground-truth labels for recommendation is good practice, because real-world applications of ML often require extracting and preprocessing data from different sources. After obtaining the data, practicing iterative ML algorithm improvement - specifically identifying breakpoints and making changes to the algorithm to test out new hypotheses - has helped me gain a better perspective of how ML works.

\section{Technical Background}
Current approaches to recommendation use two main strategies, collaborative filtering and content-based recommendations.
Collaborative filtering takes the similarity of other users
into account when linking together products and making
recommendations. For example, consider a user who enjoys playing Super Mario Bros. Collaborative filtering examines other users who also enjoy Super Mario Bros and
recommends their enjoyed entries back to the initial user.
This approach is frequently used in larger databases with
data-dense entries. Even though collaborative
filtering can provide another dimension to recommendation
than just looking at product similarity, it fundamentally requires a sense of trust that other users can and will provide
effective suggestions. This limitation is addressed in models that improve on benchmark collaborative filtering.

% cite Anthony Chow
        %Hdioud Ferdaous
        %Javier Perez-Marcos
        % Gohari for trust

By contrast, content-based recommendations examine
how different entities which are recommended can connect
together. These recommendations put products together in
semantic ways to test for similarity. One approach
involves converting string queries (e.g., game titles like "Breakout" or "Tetris") or string content (game descriptions, reviews, and other corpora) into numerical vectors and comparing their contents to vectors of related games. The TF-IDF algorithm uses sparse numerical vectors (meaning that most vector elements are 0) to identify common keywords and penalize non-informatic keywords, and approaches that use it aim to find patterns of highly informatic, low-density word usage. \cite{Meidl, Zhang, DWang} Similarly, word embedding approaches such as Word2Vec \cite{Word2Vec} or the Google Universal Sentence Encoder \cite{SentenceEncoder} use dense vectors (with mostly non-zero vector elements) to compare similarity.

FIXME Supervised learning and ML paragraph

%This section introduces the technical knowledge necessary to understand your project, including any terminology and algorithms.
%You should assume that the reader is a CS undergraduate like yourself, but not necessarily familiar with AI/ML/HCI/apps/video games/etc.

\begin{comment}
    Recommendation systems.
        Content based
        Collaborative filtering
        Some RS applying both / extension of work (e.g., ant colony optimization paper?)

        Content based via TF-IDF - examine comps proposal

        Meidl/Lytinen - cited

        Vu/Bezemer - cited

        CF

        Anthony Chow - cited
        Hdioud Ferdaous - cited
        Javier Perez-Marcos - cited

    Supervised ML.
        Machine learning
        Supervised learning
            synthetic minority oversampling
        SVM algorithm
            Relevant processes for SVM output
            Dimensionality reduction
            
    Relevant NLP content
        Word embeddings - 
\end{comment}

\section{Prior Work}

%This section describes of related and/or existing work.
%This could be scientific or scholarly, but may also be a survey of existing products/games.
%The goal of this section is to put your project in the context of what has already been done.

\begin{comment}
    Products.

        Collaborative filtering - Quantic Foundry, Steam Recommender

        cite Quantic Foundry, Steam Recommender

    Papers.

        Collaborative filtering

            Given user profiles on Steam (purchase histories and item descriptions), predict future purchases
            Choi et al. https://library.ucsd.edu/dc/object/bb5021836n/_3_1.pdf

            Gohari et al. - CF + trust to alleviate cold start issue 
            https://link.springer.com/article/10.1007/s10489-016-0830-y

        Content based

            semantic relatedness of games to match games across different contexts
            Ryan et al. https://eis.ucsc.edu/papers/ryanEtAl_PeopleTendToLikeRelatedGames.pdf

            game similarity via cosine similarity to offset popularity bias            
            Vu and Bezemer - https://asgaard.ece.ualberta.ca/wp-content/uploads/2021/04/quang_fdg2021.pdf

        Combination of above two methods

            Perez-Marcos et al. https://link.springer.com/article/10.1007/s12652-020-01681-0

        Supervised ML and its role in recommendation

            ML in RS

                Recommender systems can scrape immense amounts of data from user profiles. How to navigate and assess such voluminous data found from user profiles (CF, etc.)? Use machine learning.

                Given a MovieLens dataset, use a variety of ML approaches (supervised, unsupervised, reinforcement learning, etc.) to cluster together related movies

                Nawrocka et al. https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8399650&tag=1


                Input: user history of watch data -> SVM -> output: prediction of TV show, specifically using SVM and comparing kernel outputs
                Xu and Araki - SVM for personalized recommendations - https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1651358

                Using recommendation for sentiment analysis via ML. Anqi Zhang - https://openprairie.sdstate.edu/cgi/viewcontent.cgi?article=1487&context=etd2

                Examination of game reviews for game recs
                Meidl and Lytinen - https://ojs.aaai.org/index.php/AIIDE/article/view/12752/12600
                (read this more closely)

            NLP in RS
            TODO search "nlp for recommender system"


                Berbatova focuses on NLP techniques for Content-based RS for books
                
                Content-based RS use ML (Naive Bayes, SVM, and KNN) - very high dimensional data coming from vector representation requires NLP to smooth out

                Extract additional features from natural language data

                AND big takeaway - think of recommendation as classification task

                Berbatova - https://aclanthology.org/R19-2009.pdf

                Instead of focusing purely on NLP metrics, think about bigger picture of providing various news sources to support democracy. Can provide similar approach to recommendation (ultimate result is to give product - NLP is a means to do that)

                Reuver et al. - https://aclanthology.org/2021.hackashop-1.7.pdf

                Cluster together people's food preferences from comments and make restaurant suggestions based on that - semantic clustering from natural language and extraction of key topics

                Asani et al. - https://www.sciencedirect.com/science/article/pii/S2666827021000578 (sentiment analysis)

            "word embeddings recommender system"

            (good paper - look into more)

            To help with cold start problem, combine collaborative filtering with word embedding content analysis; express contents of vectors as embeddings of titles, genres, actors, plots

            "The main aim of this paper is to understandthe content of the movie plot using a word embedding to improve the measurementof similarity of each plot content to other plot content (called plot embedding"

            Luong Vuong Nguyen
            Tri-Hai Nguyen
            Jason J. Jung
            David Camacho

            Nguyen et al. - https://onlinelibrary.wiley.com/doi/pdf/10.1002/cpe.6232

            Use contextual information to improve RS accuracy. Propose new context-aware RS extracting context from reviews using a word embedding model - use this in addition to user profiles to get context sensitivity, tested on Yelp dataset

            (good paper - look into more)

            Sundermann et al.
            https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8609619

    
\end{comment}

\section{Methods}

%This section describes what exactly you will be working on.
%What are you building? How will it combine/incorporate ideas from the literature? Be specific about what you will be doing: talk about the specific algorithm you will implement/use, the specific dataset/platform/API, and what the outcome of your project will look like.
%All of these decisions should be justified as well.

% The approach to the project is clearly laid out, and justified with respect to literature and the goals of the project. Alternate approaches and why they are not used are discussed.

\begin{comment}
    Building a supervised learning algorithm using SVM. Given a query and a potential recommendation as input, provide whether the recommendation makes sense with respect to the query as output.

    Ideas from the literature require lot of data of user profiles and preferences (Spotify, Netflix, etc.) This is typically used in platforms that have social impact and immense user interaction. Instead, use a proxy of API recommendations as ground truth and build a model to recommend based on that ground truth. (There is plenty of precedent in the literature for ML approaches to recommendation)

    Then, the goal is to build a supervised learning problem from a ground truth dataset source. Instead of using content-based approaches and collaborative filtering, transform problem into supervised learning problem

    Approach:

    - Dataset.

    Fetched game titles, demographics (genre/theme/franchise), and deck (short form tagline) and description (long-form paragraph) from API call

    Combined Metacritic CSV and GameSpot API
    GiantBomb API

    Then created the dataset using the results of both API calls. (Combined into one big games dictionary)
    
    Iterate through games dictionary G. Let g be the current game being looked at in the games dictionary. Then get the similar games for g; call that set of similar games S. Iterate through S; store word embeddings of tokenized deck and description in X and mark each similar game (Y=1). Then iterate through all games in (G-S); store word embeddings of tokenized deck and description in X and mark each game in that iteration as Y=0.

    FIXME put in sample input PNG

    - Preprocessing.

    synthetic minority oversampling - there is a class imbalance (many more 0s than 1s). So, perform SMOTE to generate close points in N-space to balance out the classes for binary classification 

    FIXME Cite literature for SMOTE
    https://www.jair.org/index.php/jair/article/view/10302
    

    Dimensionality reduction. Since word embedding vectors can't be visualized, use principal component analysis to project the word embedding vectors into a 2D space, which allows us to conceptually understand

    FIXME Cite literature for dimensionality reduction

    - Algorithm.

    SVM is used because it scales well to high dimensional data ("One remarkable property of SVMs is that their ability to learn can be independent of the dimensionality of the feature space.") Joachims https://wiki.eecs.yorku.ca/course_archive/2013-14/W/6339/_media/joachims_98a.pdf

    Why is SVM used instead of other supervised learning algorithms?

    Logistic regression - deals with continuous data with real-world, inherently meaningful implications. For example, LogReg can be used to model housing prices given distance from the beach or highway - in this case, data is a real-world value (since it is a physical quantity of distance) and it is continuous (since we allow for arbitrarily specific measurement). On the other hand, word embedding data is not inherently meaningful - we cannot glean any physical intuition from a word embedding vector element of 0.25

    KNN - clustering is not needed for this problem. Similarity is obtained from the GiantBomb API. Because we are using the API to determine ground truth similarity, we don't need an algorithm that determines similarity via clustering or inter-cluster distance

    Decision tree / random forest. In tree-based models, we have enumerably many features that are isolated before model training. E.g., we have distance from beach and highway for housing price. In this case, natural language is used as input (deck/description), which makes it hard to isolate numerical, inherently meaningful features ahead of time.

    So,

    SVM algorithm trained on word embeddings of deck and description. The tokenized list of deck/description comes from both the game and potential recommendation. E.g.,

    query = "Breakout"
    rec = "Tetris"
    word_embedding = word embedding(query deck + query desc + rec deck + rec desc tokenized)

    X.append(word_embedding)
    y.append(0 or 1)

    FIXME put in example train and test PNG

    Why is word embedding used instead of TF-IDF? TF-IDF uses sparse word vectors, so the majority of the elements coming from the vectors are zero elements. (There are artificial positive matches between queries and potential recommendations because all vectors contain mostly 0s, so the overlap is the same even if the nonzero content is not.) By contrast, word embeddings use dense word vectors, so the majority of the elements are NOT zero elements.

    - Outcome.

    Feed query game and potential recommendation into SVM. Then the output is whether or not the game should be recommended.
    
    Example runs / testcases:
    - SVM(query = Breakout, rec = Tetris) -> 1
    - SVM(query = Breakout, rec = Baldur's Gate) -> 0

    FIXME put image of testcases from VSCode terminal

    This outcome meets the goal of the project. The idea is to use supervised learning techniques to create a ML model that can perform this recommendation task (i.e., given g and r, say if r is an appropriate recommendation for g). The testcases show sample outputs of the pipeline. 
    
\end{comment}


\section{Evaluation Metrics}

%This section describes how you will evaluate your project.
%What will you be measuring, and how will you measure it?
%You might think about what would result in an F, a C, or an A for comps.
%Alternately, think about what are the minimal requirements for passing the class, what you might do if you had more time and resources, and what the best case scenario would be if everything went swimmingly.

% The metrics used and method of collection are clearly explained and justified with respect to the literature and the goal of the project. Other metrics and why they are not used are discussed.

\begin{comment}

    Evaluation metrics:

    Problem is turned from a content-based/CF problem into a supervised learning problem. This means that immense user data and log of preferences is no longer needed. Instead, use standard ML metrics for supervised learning.

    What metrics are not being used?

    Cosine similarity. Even though cosine similarity is used frequently in the literature, it is used in cases where a "bag of words" method applies - i.e., word order and context doesn't matter, we only look at raw counts and frequencies. In this case, since paragraphs of natural language rather than simply word frequencies are considered, the bag of words assumption cannot be used.

    Metrics for recommender systems which focus on user input (liked/disliked) vs. ML numerical results

    Novelty, serendipity, etc. These are valuable when the evaluation concerns value of product (e.g., would user like the recommended game/movie/app etc.) Instead, my project is focused on maximizing TP and TN, which is a purely numerical metric independent of user input
    
    https://dl.acm.org/doi/10.1145/1864708.1864721

    ____
    See this literature:

    https://dl.acm.org/doi/abs/10.1145/1328854.1328855 - "cosine similarity has proven to be a robust metric for scoring the similarity between two strings, and it is increasingly being used in complex queries" - database management but NOT natural language processing

    FIXME check if https://ieeexplore.ieee.org/abstract/document/7349760 uses cosine similarity effectively in a way that is not relevant for my use caese

    ____
    
    
    TODO cite and apply literature that uses standard ML metrics to justify their use

    Nosayba Al-Azzam and Ibrahem Shatnawi Ph.D, PE, PMP, PTOE https://www.sciencedirect.com/science/article/pii/S2049080120305604

    Shovan Chowdhury and Marco P. Schoen https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9249211

    P. Sujatha and K. Mahalakshmi
    https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9298354
    
    
    - Standard ML metrics 
        Precision
        recall
        F-1 score
        Confusion matrix visualization


    - Testcase performance

        How well does the recommender output conform to test case output?

    - Human vs. ML output agreement?

        If a reasonably knowledgeable volunteer (i.e., self reported interest and basic schema) has similar output as algorithm output 

    Minimum viable product - pipeline is explainable and performance outperforms random guessing. (Given a class balance, random guessing is right 50% of the time - can the model outperform this?)

    More time and resources - pipeline is explainable and performance outperforms random guessing w/ agreement from several testcase suites and human volunteers

    Best case scenario: pipeline is explainable and performance outperforms random guessing w/ agreement from several testcase suites and human volunteers, and NLP approaches would be validated by human volunteers
    
\end{comment}

\section{Results and Discussion}

% The stated metrics are used and the results explained with respect to the methods. Alternate explanations and caveats to the results are explored. The results are connected to the goals of the project.

\begin{comment}

    Precision: 0.774
    Recall: 0.644
    F-1: 0.703
    Confusion matrix attached

    FIXME show confusion matrix

    Testcase performance:
    2 out of 3 passed 

    UPDATE - 3 out of 7 passed
    Breakout - Tetris - 1 - green
    Breakout - BG - 1 - red
    Super Mario Bros - The Great Giana Sisters - 1 - green

    Results make sense given methods. The metrics indicate that precision is greater than recall, implying that FP is less than FN. So, the model makes the trade-off of being more correct when its prediction is "recommendation" (lower FP) at the cost of missing some valid recommendations in total (higher FN). This makes sense when considering the training method - word embeddings of deck and description for both query and potential recommendation were used to train the SVM. So, the algorithm is more likely to hesitate if the natural language input does not clearly indicate a match between q and r, improving its precision at the cost of its recall.

    It is also possible that natural language processing input to algorithm automatically reduces recall. Even if two games have the exact same demographic characteristics (genre, theme, franchise), if the way in which their deck/description are framed is sufficiently different, the model will take that into consideration. For example, consider recommending given the first Super Mario Bros game given the original Donkey Kong arcade game. The genre (platformer) and franchise (Mario) are the same, but the descriptions may very significantly ("arcade machine" vs. "blockbuster home console video game"). In this way, an algorithm trained on word embeddings will miss what is otherwise an accurate recommendation. Future work can involve training the model beyond word embeddings so that it can capture similar qualitative aspects of the game. (this improves recall)

    On the other hand, it is also possible that further NLP approaches like sentiment analysis would also improve accuracy. By the same token, it is possible that two games have no demographic in common (different genre, theme, and franchise) but have similar text describing them. For instance, "groundbreaking 3D graphics" could describe Super Mario 64 or Gran Turismo - without context, both of those statements could be true. Considering NLP methods like dependency parsing or sentiment analysis in this case is a potential way to contextualize text so that mixups across demographics are less frequent. (this improves precision)

    Results also meet the goals of the project. Given a query game g and a potential recommendation r, the SVM outputs either a 0 (don't recommend) or a 1 (do recommend)

    Limitations include emphasis on word embeddings to train SVM algorithm. So, future work can include using NLP approaches like sentiment analysis, dependency parsing, etc. in order to further pinpoint game relatedness

    FIXME figure out how to put citations here
    
\end{comment}

%limitations

\section{Ethical Considerations}

%Are there any ethical concerns that might arise from your project?
%You might think about whether your project perpetuates societal inequity (or could be used by others to do so), whether the data/platforms you are using is collected with informed consent and free of bias, and whether you might be subject to technological solutionism instead of working support/better the public infrastructure.
%Include a discussion of how you plan to mitigate these issues in your project.

\begin{comment}
    Data bias. Like any machine learning project, the model is only as good (or as biased) as the data which is put in. The model receives data from Metacritic, GameSpot, and GiantBomb APIs. This means that it receives text data from verified reviewers who have write access to those websites, but it will not get the perspective of the average video game player. In terms of perspectives represented, then, the model overrepresents the critical perspective and ignores the average players' thoughts.

    FIXME cite Bertie Vidgen, Leon Derczynski. Directions in abu-
sive language training data, a systematic review:
Garbage in, garbage out. https://journals.
plos . org / plosone / article ? id = 10 .
1371 / journal . pone . 0243300 # sec030.
Online; accessed 9 April 2023. 2020.

While some abusive content in annotated
datasets is openly hostile or inappropriate, ”edge cases”
such as ironic statements, slander/misinformation, and in-
tention of the speaker can change the classification.

    HOW TO ADDRESS: collect reviews from as many sources as possible, not just 1 individual API. Future work if more time was involved could include scraping review content from average players (from forums, Google reviews, etc.)

    Potential for abusive content. Any NLP application which uses natural language possibly has to contend with abusive content, which includes (but is not limited to) hate speech, casual racism/sexism/other discrimination, etc. Care has to be taken to make sure that this sort of speech is not learned and casually amplified by the model.

    HOW TO ADDRESS: not as relevant in this NLP app because strictly deck/description are used (not reviews). If reviews are used, there are ways to check for hate speech 

    n example of such error checking is to
ensure that the language provided by the user is not active
hate speech. Mathew et al. provide a benchmark dataset of
hate speech, tagging classification, community involved and
rationale of speaker. [5] Performing a search to ensure that
any provided input string is not found within the hate speech
dataset is one possible way to error check before the string
is forwarded to the algorithm. 

    FIXME add paper to citation
 Binny Mathew, Punyajoy Saha, Seid Muhie Yimam,
Chris Biemann, Pawan Goyal, Animesh Mukherjee.
HateXplain: A Benchmark Dataset for Explainable
Hate Speech Detection. https : / / ojs . aaai .
org / index . php / AAAI / article / view /
17745. Online; accessed 12 April 2023.
\end{comment}

% The project is considered both in its complete technological and societal context. Issues of bias and diversity are explored in detail, and potential contributions to global and local inequity examined. Relevant literature is cited.

\section{Future Work, and Conclusion}

\begin{comment}

    Future work can involve training the model beyond word embeddings so that it can capture similar qualitative aspects of the game. (this improves recall)

    So, future work can include using NLP approaches like sentiment analysis, dependency parsing, etc. in order to further pinpoint game relatedness

    Sentiment analysis - https://www.sciencedirect.com/science/article/abs/pii/S0920548912001237

    Dependency parsing - An Intelligent Data Analysis for Recommendation Systems Using Machine Learning

    https://www.hindawi.com/journals/sp/2019/5941096/

    Future work if more time was involved could include scraping review content from average players (from forums, Google reviews, etc.)

    Conclusion:

    
    
\end{comment}

\section{Code Documentation}

This section will demonstrate that you have thought through the basics of how your code will work. You should include a diagram of the overall data flow of your program, including what the inputs and outputs of each component will be, and how they will be represented.

% Dataset constructed from name/deck/description/genre/theme/franchise obtained from API call

% Let q be the query game, and let r be the potential recommendation given the query. Let S be the set of similar games for q as obtained from GiantBomb. Then,

% Training of algorithm:
% X = word embedding(q deck + q desc + r deck + r desc), y = 0 or 1. If r in S, y=1. If r not in S, y=0

% Sample run of algorithm.
% Input: q, r
% SVM(q, r) -> 0 or 1

\section{Appendices}

\printbibliography

\end{document}

